# Machine Learning_Cappillary


Epoch 1/100, Loss: 0.4963 Epoch 2/100, Loss: 0.4534 Epoch 3/100, Loss: 0.3959 Epoch 4/100, Loss: 0.3649 Epoch 5/100, Loss: 0.2987 Epoch 6/100, Loss: 0.2911 Epoch 7/100, Loss: 0.2061 Epoch 8/100, Loss: 0.1844 Epoch 9/100, Loss: 0.1642 Epoch 10/100, Loss: 0.1572 Epoch 11/100, Loss: 0.1578 Epoch 12/100, Loss: 0.1363 Epoch 13/100, Loss: 0.1108 Epoch 14/100, Loss: 0.1350 Epoch 15/100, Loss: 0.0846 Epoch 16/100, Loss: 0.1264 Epoch 17/100, Loss: 0.1076 Epoch 18/100, Loss: 0.0675 Epoch 19/100, Loss: 0.1007 Epoch 20/100, Loss: 0.0757 Epoch 21/100, Loss: 0.1187 Epoch 22/100, Loss: 0.1244 Epoch 23/100, Loss: 0.0608 Epoch 24/100, Loss: 0.1133 Epoch 25/100, Loss: 0.0827 Epoch 26/100, Loss: 0.0811 Epoch 27/100, Loss: 0.0576 Epoch 28/100, Loss: 0.0868 Epoch 29/100, Loss: 0.0881 Epoch 30/100, Loss: 0.1034 Epoch 31/100, Loss: 0.0848 Epoch 32/100, Loss: 0.0563 Epoch 33/100, Loss: 0.0535 Epoch 34/100, Loss: 0.0823 Epoch 35/100, Loss: 0.0706 Epoch 36/100, Loss: 0.0698 Epoch 37/100, Loss: 0.0892 Epoch 38/100, Loss: 0.0580 Epoch 39/100, Loss: 0.0478 Epoch 40/100, Loss: 0.0621 Epoch 41/100, Loss: 0.0361 Epoch 42/100, Loss: 0.0461 Epoch 43/100, Loss: 0.0732 Epoch 44/100, Loss: 0.0452 Epoch 45/100, Loss: 0.0837 Epoch 46/100, Loss: 0.0536 Epoch 47/100, Loss: 0.0636 Epoch 48/100, Loss: 0.0237 Epoch 49/100, Loss: 0.0204 Epoch 50/100, Loss: 0.0143 Epoch 51/100, Loss: 0.0118 Epoch 52/100, Loss: 0.0137 Epoch 53/100, Loss: 0.0122 Epoch 54/100, Loss: 0.0088 Epoch 55/100, Loss: 0.0088 Epoch 56/100, Loss: 0.0089 Epoch 57/100, Loss: 0.0041 Epoch 58/100, Loss: 0.0067 Epoch 59/100, Loss: 0.0080 Epoch 60/100, Loss: 0.0078 Epoch 61/100, Loss: 0.0028 Epoch 62/100, Loss: 0.0083 Epoch 63/100, Loss: 0.0125 Epoch 64/100, Loss: 0.0113 Epoch 65/100, Loss: 0.0036 Epoch 66/100, Loss: 0.0061 Epoch 67/100, Loss: 0.0029 Epoch 68/100, Loss: 0.0040 Epoch 69/100, Loss: 0.0097 Epoch 70/100, Loss: 0.0035 Epoch 71/100, Loss: 0.0047

---
## ResNet50 Accuracy : 88%
---

## hyperprameter 
Batch Size = 16


Learning Rate = 0.001 + Learning Rate Scheduler(5 epoch 동안 손실이 개선되지 않으면 학습률 감소)


Optimizer = Adam


Dropout Rate = 0.5


Batch Normalization 추가


Epochs = 100


Early stopping = 10


![image](https://github.com/user-attachments/assets/7b07ab81-99b1-4d3d-b982-a4686a74351f)

## example Label data
![image](https://github.com/user-attachments/assets/cf8a2b28-078f-4c49-b56d-acce6fc78e88)

## ResNet50_Loss
![image](https://github.com/user-attachments/assets/0a40b670-37a6-4380-90d1-4add902b079f)

## Confusion Matric
![image](https://github.com/user-attachments/assets/e640f5a5-4199-4b73-973c-07826d9031d1)

# Result
![image](https://github.com/user-attachments/assets/df2dcc8a-9368-4ab4-909b-ffcd350211cc)



